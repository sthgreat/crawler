# coding=utf-8
from selenium import webdriver
import csv
from time import sleep
import requests
from bs4 import BeautifulSoup
import bs4
import re
import time

def mkdir(path):
    import os
    path = path.strip()
    path = path.rstrip("\\")
    isExists = os.path.exists(path)

    if not isExists:
        os.makedirs(path)
    else:
        return False

def get_information_and_save(path, path1, path2, html, key):
    sleep(1)
    mkdir(path)
    u = re.findall(r'天眼查为您提供.*?信息查询', html)
    u1 = re.sub('天|眼|查|为|您|提|供|工|商|信|息|查|询', '', str(u))
    num = re.findall(r'工商注册号.*?组织机构代码', html)
    num1 = re.findall(r'\d{5,30}', str(num))
    addr = re.findall(r'注册地址：.*?!', html)
    addr1 = re.findall(r'<td.*<!', str(addr))
    addr2 = re.sub('<|>|td|colspan|=|"|\d{1}|!', '', str(addr1))
    if u1 == key:
        #with open(path1, 'w', newline='') as csvfile:
            #writer = csv.writer(csvfile)
            #writer.writerow([key, u1, num1, addr2] + ['\n'])
        file = open(path1, mode='a')
        file.write('\n' + '{}\t{}\t{}\t{}'.format(key, u1, str(num1), str(addr2)) + '\n')
        file.close
    else:
        #with open(path2, 'w', newline='') as csvfile:
           # for num, i in enumerate(path2):
               # if (num > k) and (num <= k + 1):
                  #  writer = csv.writer(csvfile)
                  #  writer.writerow([key, u1, num1, addr2])
        file = open(path2, mode='a')
        file.write('{},{},{},{}'.format(key, u1, str(num1), str(addr2)) + '\n')
        file.close

def main():
    k = 0
    count = 0
    path = 'D:/pics/公司+工商号'
    path1 = path + '/全称公司1.txt'
    path2 = path + '/非全称公司2.txt'
    first_key = '未收集'
    wd = webdriver.Firefox()
    dzkdLogInUrl = 'https://www.tianyancha.com/login'
    wd.get(dzkdLogInUrl)
    wd.find_element_by_xpath('//*[@id="bannerClose"]').click()  #关闭遮挡元素
    wd.find_element_by_xpath('//*[@id="web-content"]/div/div/div/div[2]/div/div[2]/div[2]/div[2]/div[2]/input').send_keys('18482167217')
    wd.find_element_by_xpath('//*[@id="web-content"]/div/div/div/div[2]/div/div[2]/div[2]/div[2]/div[3]/input').send_keys('19960409jsb')
    wd.find_element_by_xpath('//*[@id="web-content"]/div/div/div/div[2]/div/div[2]/div[2]/div[2]/div[5]').click()
    sleep(2)  # 等待Cookies加载
    wd.find_element_by_xpath('//*[@id="home-main-search"]').send_keys(first_key)
    wd.find_element_by_xpath('//*[@id="web-content"]/div/div[1]/div[2]/div/div[2]/div[2]/div/div[1]/div[1]/div').click()  #进入二级检索页面，进入循环
    reader = csv.reader(open('D:\pics\厂家\公司上.csv', encoding='utf-8'))
    for num, i in enumerate(reader):
        if num >= 1:
            sleep(1.5)
            #wd.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[2]/div[1]/div[1]').click()
            wd.find_element_by_xpath('//*[@id="header-company-search"]').clear()
            wd.find_element_by_xpath('//*[@id="header-company-search"]').send_keys(str(i[0]))
            wd.find_element_by_css_selector('#web-header > div > div > div.head-left > div.head-tab-outer > div.head-tab-body.head-tab-company > div.input-group.search_group.head-search-group-company > div').click()
            sleep(2)
            judge = wd.page_source
            judge1 = re.findall(r'抱歉', judge)
            if judge1 == []:
                sleep(0.5)
                first_window = wd.current_window_handle
                sleep(1)
                wd.find_element_by_css_selector('#searchTogal').click()  #收起小框
                wd.find_element_by_css_selector('div.search_result_single:nth-child(1) > div:nth-child(2) > div:nth-child(1) > a:nth-child(1)').click()  #点击搜索页面第一个，打开第二个页面
                sleep(0.5)
                handles = wd.window_handles
                wd.close()
                for handle in handles:    #转移句柄
                    if handle != first_window:
                        wd.switch_to.window(handle)
                sleep(5)  #等待加载页面
                html = wd.page_source
                get_information_and_save(path, path1, path2, html, str(i[0]))
                count = count + 1
                print(count)
            else:
                sleep(1)
                wd.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[2]/div[1]/div[1]').click()
                wd.find_element_by_xpath('//*[@id="header-company-search"]').send_keys(str(i[0]))
                wd.find_element_by_xpath('//*[@id="web-header"]/div/div/div[1]/div[2]/div[2]/div[1]/div').click()
                count = count + 1
                print(count)

main()
